{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Caching Strategies for GPT-OSS\n",
        "\n",
        "This notebook demonstrates different ways to keep the GPT-OSS model loaded and responsive.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime\n",
        "\n",
        "# Configuration\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434\"\n",
        "MODEL_NAME = \"gpt-oss:20b\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 1: Check current model status\n",
        "def check_model_status():\n",
        "    \"\"\"Check if model is currently loaded\"\"\"\n",
        "    try:\n",
        "        response = requests.get(f\"{OLLAMA_BASE_URL}/api/ps\")\n",
        "        if response.status_code == 200:\n",
        "            models = response.json()\n",
        "            print(\"Currently loaded models:\")\n",
        "            if models.get('models'):\n",
        "                for model in models['models']:\n",
        "                    print(f\"  - {model['name']} (Size: {model.get('size_vram', 'Unknown')})\")\n",
        "                    print(f\"    Until: {model.get('expires_at', 'Unknown')}\")\n",
        "            else:\n",
        "                print(\"  No models currently loaded\")\n",
        "        else:\n",
        "            print(\"Could not check model status\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking model status: {e}\")\n",
        "\n",
        "check_model_status()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Method 2: Preload model with keep-alive setting\n",
        "def preload_model(keep_alive=\"30m\"):\n",
        "    \"\"\"Preload the model and set keep-alive duration\"\"\"\n",
        "    print(f\"Preloading {MODEL_NAME} with keep-alive: {keep_alive}\")\n",
        "    \n",
        "    payload = {\n",
        "        \"model\": MODEL_NAME,\n",
        "        \"keep_alive\": keep_alive,  # Keep model loaded for 30 minutes\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Hello, please confirm you're ready.\"}\n",
        "        ],\n",
        "        \"stream\": False\n",
        "    }\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(f\"{OLLAMA_BASE_URL}/api/chat\", json=payload, timeout=120)\n",
        "        load_time = time.time() - start_time\n",
        "        \n",
        "        if response.status_code == 200:\n",
        "            result = response.json()\n",
        "            print(f\"✅ Model loaded successfully in {load_time:.2f}s\")\n",
        "            print(f\"Response: {result['message']['content'][:100]}...\")\n",
        "            print(f\"Model will stay loaded until: {datetime.now().strftime('%H:%M')} + {keep_alive}\")\n",
        "        else:\n",
        "            print(f\"❌ Failed to load model: {response.status_code}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"Error preloading model: {e}\")\n",
        "\n",
        "# Preload with 30 minute keep-alive\n",
        "preload_model(\"30m\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
