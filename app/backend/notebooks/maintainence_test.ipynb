{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "592c48eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the parent directory to Python path so we can import from core\n",
    "import sys\n",
    "import os\n",
    "# Get the current working directory and go up one level to the backend directory\n",
    "backend_dir = os.path.dirname(os.getcwd())\n",
    "if backend_dir not in sys.path:\n",
    "    sys.path.append(backend_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b57c10f",
   "metadata": {},
   "outputs": [
    {
     "ename": "CancelledError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCancelledError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m      8\u001b[39m diagnosis: DiagnosisProbability = {\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdiagnosis\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mLow coolant level / coolant leak â€” triggering overheat protection\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m: \u001b[32m0.82\u001b[39m\n\u001b[32m     11\u001b[39m }\n\u001b[32m     13\u001b[39m diagnosis_history = [\n\u001b[32m     14\u001b[39m     {\n\u001b[32m     15\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mtest_0\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     40\u001b[39m     }\n\u001b[32m     41\u001b[39m ]\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m result = \u001b[38;5;28;01mawait\u001b[39;00m maintainence_agent.run(\n\u001b[32m     44\u001b[39m     diagnosis_history[:\u001b[32m2\u001b[39m],\n\u001b[32m     45\u001b[39m     diagnosis, \n\u001b[32m     46\u001b[39m     diagnosis_history,\n\u001b[32m     47\u001b[39m )\n\u001b[32m     49\u001b[39m \u001b[38;5;28mprint\u001b[39m(result, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\core\\agents\\maintainence.py:142\u001b[39m, in \u001b[36mMaintainenceAgent.run\u001b[39m\u001b[34m(self, problem_description, diagnosis, diagnosis_history)\u001b[39m\n\u001b[32m    136\u001b[39m llm_messages = [\n\u001b[32m    137\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m.SYSTEM_PROMPT},\n\u001b[32m    138\u001b[39m     {\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: user_prompt}\n\u001b[32m    139\u001b[39m ]\n\u001b[32m    141\u001b[39m _final_answer_chunks: List[\u001b[38;5;28mstr\u001b[39m] = []\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat(\n\u001b[32m    143\u001b[39m     messages=llm_messages,\n\u001b[32m    144\u001b[39m     think=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    145\u001b[39m ):\n\u001b[32m    146\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk.get(\u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    147\u001b[39m         \u001b[38;5;28mprint\u001b[39m(chunk[\u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m], end=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\core\\llm.py:54\u001b[39m, in \u001b[36mLLMClient.chat\u001b[39m\u001b[34m(self, messages, keep_alive, think, chat_params)\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchat\u001b[39m(\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     45\u001b[39m     messages: List[Dict[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mstr\u001b[39m]],\n\u001b[32m   (...)\u001b[39m\u001b[32m     48\u001b[39m     chat_params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = _default_chat_params,\n\u001b[32m     49\u001b[39m ) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m     50\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     51\u001b[39m \u001b[33;03m    Send a chat request to the LLM.\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m54\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m part \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.client.chat(\n\u001b[32m     55\u001b[39m         model=\u001b[38;5;28mself\u001b[39m.model,\n\u001b[32m     56\u001b[39m         messages=messages,\n\u001b[32m     57\u001b[39m         stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     58\u001b[39m         keep_alive=keep_alive,\n\u001b[32m     59\u001b[39m         think=think,\n\u001b[32m     60\u001b[39m     ):\n\u001b[32m     61\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m {\n\u001b[32m     62\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: part[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33massistant\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     63\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m: part[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mthinking\u001b[39m\u001b[33m\"\u001b[39m),   \u001b[38;5;66;03m# <-- reasoning text (may be None)\u001b[39;00m\n\u001b[32m     64\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: part[\u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m),     \u001b[38;5;66;03m# <-- final answer tokens\u001b[39;00m\n\u001b[32m     65\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m: part.get(\u001b[33m\"\u001b[39m\u001b[33mdone\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[32m     66\u001b[39m         }\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\ollama\\_client.py:677\u001b[39m, in \u001b[36mAsyncClient._request.<locals>.inner\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    676\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m():\n\u001b[32m--> \u001b[39m\u001b[32m677\u001b[39m   \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._client.stream(*args, **kwargs) \u001b[38;5;28;01mas\u001b[39;00m r:\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    679\u001b[39m       r.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\contextlib.py:210\u001b[39m, in \u001b[36m_AsyncGeneratorContextManager.__aenter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args, \u001b[38;5;28mself\u001b[39m.kwds, \u001b[38;5;28mself\u001b[39m.func\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m210\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m anext(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopAsyncIteration\u001b[39;00m:\n\u001b[32m    212\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mgenerator didn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt yield\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_client.py:1583\u001b[39m, in \u001b[36mAsyncClient.stream\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m   1560\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1561\u001b[39m \u001b[33;03mAlternative to `httpx.request()` that streams the response body\u001b[39;00m\n\u001b[32m   1562\u001b[39m \u001b[33;03minstead of loading it into memory at once.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1568\u001b[39m \u001b[33;03m[0]: /quickstart#streaming-responses\u001b[39;00m\n\u001b[32m   1569\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1570\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m   1571\u001b[39m     method=method,\n\u001b[32m   1572\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1581\u001b[39m     extensions=extensions,\n\u001b[32m   1582\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1583\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.send(\n\u001b[32m   1584\u001b[39m     request=request,\n\u001b[32m   1585\u001b[39m     auth=auth,\n\u001b[32m   1586\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1587\u001b[39m     stream=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1588\u001b[39m )\n\u001b[32m   1589\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1590\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_client.py:1629\u001b[39m, in \u001b[36mAsyncClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m   1625\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m   1627\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m-> \u001b[39m\u001b[32m1629\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_auth(\n\u001b[32m   1630\u001b[39m     request,\n\u001b[32m   1631\u001b[39m     auth=auth,\n\u001b[32m   1632\u001b[39m     follow_redirects=follow_redirects,\n\u001b[32m   1633\u001b[39m     history=[],\n\u001b[32m   1634\u001b[39m )\n\u001b[32m   1635\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1636\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_client.py:1657\u001b[39m, in \u001b[36mAsyncClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m   1654\u001b[39m request = \u001b[38;5;28;01mawait\u001b[39;00m auth_flow.\u001b[34m__anext__\u001b[39m()\n\u001b[32m   1656\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1657\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_handling_redirects(\n\u001b[32m   1658\u001b[39m         request,\n\u001b[32m   1659\u001b[39m         follow_redirects=follow_redirects,\n\u001b[32m   1660\u001b[39m         history=history,\n\u001b[32m   1661\u001b[39m     )\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1663\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_client.py:1694\u001b[39m, in \u001b[36mAsyncClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m   1691\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m   1692\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m hook(request)\n\u001b[32m-> \u001b[39m\u001b[32m1694\u001b[39m response = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._send_single_request(request)\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1696\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_client.py:1730\u001b[39m, in \u001b[36mAsyncClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1725\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1726\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an sync request with an AsyncClient instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1727\u001b[39m     )\n\u001b[32m   1729\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1730\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m transport.handle_async_request(request)\n\u001b[32m   1732\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, AsyncByteStream)\n\u001b[32m   1733\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpx\\_transports\\default.py:394\u001b[39m, in \u001b[36mAsyncHTTPTransport.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    381\u001b[39m req = httpcore.Request(\n\u001b[32m    382\u001b[39m     method=request.method,\n\u001b[32m    383\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    391\u001b[39m     extensions=request.extensions,\n\u001b[32m    392\u001b[39m )\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m394\u001b[39m     resp = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pool.handle_async_request(req)\n\u001b[32m    396\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.AsyncIterable)\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    399\u001b[39m     status_code=resp.status,\n\u001b[32m    400\u001b[39m     headers=resp.headers,\n\u001b[32m    401\u001b[39m     stream=AsyncResponseStream(resp.stream),\n\u001b[32m    402\u001b[39m     extensions=resp.extensions,\n\u001b[32m    403\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:256\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.AsyncIterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_async\\connection_pool.py:236\u001b[39m, in \u001b[36mAsyncConnectionPool.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = \u001b[38;5;28;01mawait\u001b[39;00m pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[38;5;28;01mawait\u001b[39;00m connection.handle_async_request(\n\u001b[32m    237\u001b[39m         pool_request.request\n\u001b[32m    238\u001b[39m     )\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:101\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection.handle_async_request(request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:78\u001b[39m, in \u001b[36mAsyncHTTPConnection.handle_async_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     76\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._request_lock:\n\u001b[32m     77\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m         stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._connect(request)\n\u001b[32m     80\u001b[39m         ssl_object = stream.get_extra_info(\u001b[33m\"\u001b[39m\u001b[33mssl_object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     81\u001b[39m         http2_negotiated = (\n\u001b[32m     82\u001b[39m             ssl_object \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     83\u001b[39m             \u001b[38;5;129;01mand\u001b[39;00m ssl_object.selected_alpn_protocol() == \u001b[33m\"\u001b[39m\u001b[33mh2\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     84\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_async\\connection.py:124\u001b[39m, in \u001b[36mAsyncHTTPConnection._connect\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    116\u001b[39m     kwargs = {\n\u001b[32m    117\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhost\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.host.decode(\u001b[33m\"\u001b[39m\u001b[33mascii\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    118\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mport\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._origin.port,\n\u001b[32m   (...)\u001b[39m\u001b[32m    121\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msocket_options\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m._socket_options,\n\u001b[32m    122\u001b[39m     }\n\u001b[32m    123\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mconnect_tcp\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m124\u001b[39m         stream = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._network_backend.connect_tcp(**kwargs)\n\u001b[32m    125\u001b[39m         trace.return_value = stream\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_backends\\auto.py:31\u001b[39m, in \u001b[36mAutoBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconnect_tcp\u001b[39m(\n\u001b[32m     23\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m     24\u001b[39m     host: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     28\u001b[39m     socket_options: typing.Iterable[SOCKET_OPTION] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     29\u001b[39m ) -> AsyncNetworkStream:\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._init_backend()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.connect_tcp(\n\u001b[32m     32\u001b[39m         host,\n\u001b[32m     33\u001b[39m         port,\n\u001b[32m     34\u001b[39m         timeout=timeout,\n\u001b[32m     35\u001b[39m         local_address=local_address,\n\u001b[32m     36\u001b[39m         socket_options=socket_options,\n\u001b[32m     37\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\httpcore\\_backends\\anyio.py:115\u001b[39m, in \u001b[36mAnyIOBackend.connect_tcp\u001b[39m\u001b[34m(self, host, port, timeout, local_address, socket_options)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    114\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m anyio.fail_after(timeout):\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m         stream: anyio.abc.ByteStream = \u001b[38;5;28;01mawait\u001b[39;00m anyio.connect_tcp(\n\u001b[32m    116\u001b[39m             remote_host=host,\n\u001b[32m    117\u001b[39m             remote_port=port,\n\u001b[32m    118\u001b[39m             local_host=local_address,\n\u001b[32m    119\u001b[39m         )\n\u001b[32m    120\u001b[39m         \u001b[38;5;66;03m# By default TCP sockets opened in `asyncio` include TCP_NODELAY.\u001b[39;00m\n\u001b[32m    121\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m option \u001b[38;5;129;01min\u001b[39;00m socket_options:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\anyio\\_core\\_sockets.py:215\u001b[39m, in \u001b[36mconnect_tcp\u001b[39m\u001b[34m(remote_host, remote_port, local_host, tls, ssl_context, tls_standard_compatible, tls_hostname, happy_eyeballs_delay)\u001b[39m\n\u001b[32m    212\u001b[39m         target_addrs = [(socket.AF_INET, addr_obj.compressed)]\n\u001b[32m    213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    214\u001b[39m     \u001b[38;5;66;03m# getaddrinfo() will raise an exception if name resolution fails\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m     gai_res = \u001b[38;5;28;01mawait\u001b[39;00m getaddrinfo(\n\u001b[32m    216\u001b[39m         target_host, remote_port, family=family, \u001b[38;5;28mtype\u001b[39m=socket.SOCK_STREAM\n\u001b[32m    217\u001b[39m     )\n\u001b[32m    219\u001b[39m     \u001b[38;5;66;03m# Organize the list so that the first address is an IPv6 address (if available)\u001b[39;00m\n\u001b[32m    220\u001b[39m     \u001b[38;5;66;03m# and the second one is an IPv4 addresses. The rest can be in whatever order.\u001b[39;00m\n\u001b[32m    221\u001b[39m     v6_found = v4_found = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\anyio\\_core\\_sockets.py:592\u001b[39m, in \u001b[36mgetaddrinfo\u001b[39m\u001b[34m(host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    590\u001b[39m     encoded_host = host\n\u001b[32m--> \u001b[39m\u001b[32m592\u001b[39m gai_res = \u001b[38;5;28;01mawait\u001b[39;00m get_async_backend().getaddrinfo(\n\u001b[32m    593\u001b[39m     encoded_host, port, family=family, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mtype\u001b[39m, proto=proto, flags=flags\n\u001b[32m    594\u001b[39m )\n\u001b[32m    595\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    596\u001b[39m     (family, \u001b[38;5;28mtype\u001b[39m, proto, canonname, convert_ipv6_sockaddr(sockaddr))\n\u001b[32m    597\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m family, \u001b[38;5;28mtype\u001b[39m, proto, canonname, sockaddr \u001b[38;5;129;01min\u001b[39;00m gai_res\n\u001b[32m    598\u001b[39m     \u001b[38;5;66;03m# filter out IPv6 results when IPv6 is disabled\u001b[39;00m\n\u001b[32m    599\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sockaddr[\u001b[32m0\u001b[39m], \u001b[38;5;28mint\u001b[39m)\n\u001b[32m    600\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jakej\\Files\\Projects\\OpenAI gpt-oss hackathon\\dashtech\\app\\backend\\venv\\Lib\\site-packages\\anyio\\_backends\\_asyncio.py:2698\u001b[39m, in \u001b[36mAsyncIOBackend.getaddrinfo\u001b[39m\u001b[34m(cls, host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m   2679\u001b[39m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[32m   2680\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetaddrinfo\u001b[39m(\n\u001b[32m   2681\u001b[39m     \u001b[38;5;28mcls\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2696\u001b[39m     ]\n\u001b[32m   2697\u001b[39m ]:\n\u001b[32m-> \u001b[39m\u001b[32m2698\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m get_running_loop().getaddrinfo(\n\u001b[32m   2699\u001b[39m         host, port, family=family, \u001b[38;5;28mtype\u001b[39m=\u001b[38;5;28mtype\u001b[39m, proto=proto, flags=flags\n\u001b[32m   2700\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\asyncio\\base_events.py:901\u001b[39m, in \u001b[36mBaseEventLoop.getaddrinfo\u001b[39m\u001b[34m(self, host, port, family, type, proto, flags)\u001b[39m\n\u001b[32m    898\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    899\u001b[39m     getaddr_func = socket.getaddrinfo\n\u001b[32m--> \u001b[39m\u001b[32m901\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.run_in_executor(\n\u001b[32m    902\u001b[39m     \u001b[38;5;28;01mNone\u001b[39;00m, getaddr_func, host, port, family, \u001b[38;5;28mtype\u001b[39m, proto, flags)\n",
      "\u001b[31mCancelledError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from core.agents.diagnostics import DiagnosisProbability\n",
    "from core.agents.maintainence import MaintainenceAgent\n",
    "from core.llm import LLMClient\n",
    "\n",
    "llm_client = LLMClient()\n",
    "maintainence_agent = MaintainenceAgent(llm_client)\n",
    "\n",
    "diagnosis: DiagnosisProbability = {\n",
    "    \"diagnosis\": \"Low coolant level / coolant leak â€” triggering overheat protection\",\n",
    "    \"probability\": 0.82\n",
    "}\n",
    "\n",
    "diagnosis_history = [\n",
    "    {\n",
    "        \"id\": \"test_0\",\n",
    "        \"name\": \"Issue Description\",\n",
    "        \"description\": \"Initial user report\",\n",
    "        \"rationale\": \"\",\n",
    "        \"outcomes\": {},\n",
    "        \"result\": \"Red warning light: coolant level low.\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_1\",\n",
    "        \"name\": \"Error Codes Present\",\n",
    "        \"description\": \"Scan VIC for coolant level faults.\",\n",
    "        \"rationale\": \"22901-22/23 relate to coolant level sensor out of range.\",\n",
    "        \"outcomes\": {},\n",
    "        \"result\": [\"22901-22\"]\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"test_2\",\n",
    "        \"name\": \"Coolant Level & Leak Check\",\n",
    "        \"description\": \"Inspect expansion tank, hoses, and radiator for leaks.\",\n",
    "        \"rationale\": \"Leaks or evaporation are common causes for warnings.\",\n",
    "        \"outcomes\": {\n",
    "            \"type_of_outcome\": \"string\",\n",
    "            \"outcome_data\": \"possible values: 'tank low, no leak', 'tank low, hose leak', 'level OK'\"\n",
    "        },\n",
    "        \"result\": \"Tank low, hose leak visible\"\n",
    "    }\n",
    "]\n",
    "\n",
    "result = await maintainence_agent.run(\n",
    "    diagnosis_history[:2],\n",
    "    diagnosis, \n",
    "    diagnosis_history,\n",
    ")\n",
    "\n",
    "print(result, flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
